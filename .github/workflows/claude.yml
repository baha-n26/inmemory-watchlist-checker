name: Claude PR Assistant (Bedrock)

on:
  issue_comment:            { types: [created] }
  pull_request_review_comment: { types: [created] }
  pull_request_review:      { types: [submitted] }
  issues:                   { types: [opened, assigned] }
  pull_request:             { types: [opened, synchronize] }

jobs:
  ###########################################################################
  # 1.  MANUAL â€œ@claude â€¦â€  â€“ only fires when the tag is present in a comment
  ###########################################################################
  claude-manual:
    if: |
      (github.event_name == 'issue_comment' &&
       contains(github.event.comment.body, '@claude')) ||
      (github.event_name == 'pull_request_review_comment' &&
       contains(github.event.comment.body, '@claude')) ||
      (github.event_name == 'pull_request_review' &&
       contains(github.event.review.body, '@claude')) ||
      (github.event_name == 'issues' &&
       contains(github.event.issue.body, '@claude'))

    runs-on: ubuntu-latest

    permissions:            # <- now includes what the action needs
      contents:        read
      pull-requests:   write   # let it leave review comments
      issues:          write   # let it reply on issues
      id-token:        write   # not strictly needed because we pass github_token,
      # but harmless and future-proof

    env:                       # static AWS credentials for Bedrock
      AWS_ACCESS_KEY_ID:     ${{ secrets.AWS_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      AWS_SESSION_TOKEN:     ${{ secrets.AWS_SESSION_TOKEN }}   # optional
      AWS_REGION:            eu-central-1

    steps:
      - uses: actions/checkout@v4

      - name: Claude (manual) via Bedrock
        uses: anthropics/claude-code-action@beta
        with:
          github_token: ${{ github.token }}   # <- fixes the OIDC error
          use_bedrock: "true"
          model: arn:aws:bedrock:eu-central-1:538639307912:application-inference-profile/bsfdqs400k7z
          trigger_phrase: "@claude"
          timeout_minutes: "60"

  ###############################################################
  # 2.  AUTOMATIC REVIEW â€“ runs on every push / open to a PR
  ###############################################################
  claude-auto-review:
    if: github.event_name == 'pull_request'
    runs-on: ubuntu-latest

    permissions:
      contents:        read
      pull-requests:   write   # writes the review
      id-token:        write   # optional when github_token supplied

    env:
      AWS_ACCESS_KEY_ID:     ${{ secrets.AWS_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      AWS_SESSION_TOKEN:     ${{ secrets.AWS_SESSION_TOKEN }}
      AWS_REGION:            eu-central-1

    steps:
      - uses: actions/checkout@v4

      - name: Claude PR Review via Bedrock
        uses: anthropics/claude-code-action@beta
        with:
          github_token: ${{ github.token }}   # â† avoids OIDC path
          use_bedrock: "true"
          model: arn:aws:bedrock:eu-central-1:538639307912:application-inference-profile/bsfdqs400k7z
          timeout_minutes: "60"

          # ------------- your custom review prompt -------------
direct_prompt: |
  You are the automated code-review assistant for **Kotlin API v2.1**, a backend service that:
    â€¢ Reads a large (1.3 GB) CSV of sanctioned and blacklisted individuals (millions of rows).
    â€¢ Builds in-memory Lucene indexes for fast lookups.
    â€¢ Exposes a **Sanctioned Person Checker** Service to query by name, date-of-birth, or source list.
    â€¢ Applies Jaro-Winkler similarity scoring and returns a hit only if above our project threshold.
    â€¢ Must keep search latency < 500ms under peak (~200 reqs/sec) and stay within heap limits.
    â€¢ Csv input, parsing, and indexing are handled in `src/main/kotlin/watchlist/`.
    â€¢ Includes benchmark tests for both memory usage and search speed (in `src/jmh/kotlin/benchmark/`).

  **Focus Areas:**
  - **Performance & Scalability:** Ensure Lucene queries, lucene indexes, and similarity scoring meet <c500cms SLA.
  - **Memory Management:** Verify in-memory indexes stay within configured heap limits; check benchmark results.
  - **Similarity Logic:** Validate Jaro-Winkler threshold logic and avoid false positives/negatives.
  - **Index Integrity:** No data loss in CSV parsing; proper handling of nulls, encoding, and date formats.

  ## 1. PR Summary & Scope
  â€¢ Summarize the intent (new parser, index format change, search enhancements).
  â€¢ Highlight any CSV schema or Lucene index modifications.

  ## 2. Architecture & Indexing
  â€¢ Confirm separation of CSV parsing, indexing, and search modules (`.parser`, `.indexes`, `.api`).
  â€¢ Ensure Lucene `IndexWriter`/`Directory` usage is thread-safe and properly closed.

  ## 3. Search API & Contracts
  â€¢ Check request/response models in any controllers, ensure consistent API envelopes.
  â€¢ Filter parameters (DOB, source) must use correct types and validation annotations.

  ## 4. Performance & Benchmarks
  â€¢ Review benchmark configs in `benchmarks/`, ensure they reflect production heap and thread-pool settings.
  â€¢ Identify any potential hotspots or unbounded memory growth in index structures.

  ## 5. Similarity & Accuracy
  â€¢ Verify Jaro-Winkler or any other added Similiarty Scoring implementation and threshold constants.
  â€¢ Check unit tests around edge cases (name variations, diacritics, casing).

  ## 6. Testing & Reliability
  â€¢ Confirm CSV parsing tests cover large-file streaming without OOM.
  â€¢ Ensure integration tests include search under simulated load (e.g., using Gatling).

  ## 7. Observability & Monitoring
  â€¢ Ensure metrics (search latency, index size) are emitted via Micrometer timers/gauges.
  â€¢ Logs for parse errors and search timeouts are present.

  ## 8. Security & Data Handling
  â€¢ Validate input sanitization to avoid injection in Lucene queries.
  â€¢ Ensure no raw CSV path exposure; secrets/config in `application.yml`.

  ## 9. Documentation & ADRs
  â€¢ Update README with CSV source, index build steps, and benchmark instructions.
  â€¢ Add or update ADR if altering indexing strategy or similarity algorithm.

  ## 10. Actionable Feedback
  â€¢ List findings as ğŸ”´/ğŸŸ /ğŸŸ¢ with file paths or snippets, e.g.
    ğŸ”´ `CSVParser.kt: potential OOM reading entire file into memory.`

  ## 11. Verdict
  â€¢ APPROVE / REQUEST_CHANGES / COMMENT_ONLY
  â€¢ Brief rationale (e.g., â€œREQUEST_CHANGES: similarity threshold edge-case missing testâ€).
  
  Please keep the tone **constructive**, **concise**, and output **Markdown** only.
