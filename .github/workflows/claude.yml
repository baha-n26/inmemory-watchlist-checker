name: Claude PR Assistant

on:
  # Manual ‚Äú@claude ‚Ä¶‚Äù triggers
  issue_comment:            { types: [created] }
  pull_request_review_comment: { types: [created] }
  pull_request_review:      { types: [submitted] }
  issues:                   { types: [opened, assigned] }

  # Automatic review on every PR push / creation
  pull_request:             { types: [opened, synchronize] }

############################
# 1. MANUAL ‚Äú@claude ‚Ä¶‚Äù JOB #
############################
jobs:
  claude-manual:
    if: |
      (github.event_name == 'issue_comment' &&
       contains(github.event.comment.body, '@claude')) ||
      (github.event_name == 'pull_request_review_comment' &&
       contains(github.event.comment.body, '@claude')) ||
      (github.event_name == 'pull_request_review' &&
       contains(github.event.review.body, '@claude')) ||
      (github.event_name == 'issues' &&
       contains(github.event.issue.body, '@claude'))

    runs-on: ubuntu-latest
    permissions:
      contents: read
      pull-requests: read
      issues: read
      id-token: write

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 1

      - name: Run Claude (manual trigger)
        uses: anthropics/claude-code-action@beta
        with:
          anthropic_api_key: ${{ secrets.ANTHROPIC_API_KEY }}
          model: claude-opus-4-20250514
          timeout_minutes: "60"
          trigger_phrase: "@claude"

##########################
# 2. AUTOMATIC PR REVIEW #
##########################
  claude-auto-review:
    if: github.event_name == 'pull_request'
    runs-on: ubuntu-latest
    permissions:
      contents: read
      pull-requests: read
      id-token: write

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 1

      - name: Claude Automatic PR Review
        uses: anthropics/claude-code-action@beta
        with:
          anthropic_api_key: ${{ secrets.ANTHROPIC_API_KEY }}
          model: claude-opus-4-20250514
          timeout_minutes: "60"
          
   # ===== Project-specific structured review prompt =====
direct_prompt: |
  You are the automated code-review assistant for **Kotlin API v2.1**, a backend service that:
    ‚Ä¢ Reads a large (1.3 GB) CSV of sanctioned and blacklisted individuals (millions of rows).
    ‚Ä¢ Builds in-memory Lucene indexes for fast lookups.
    ‚Ä¢ Exposes a **Sanctioned Person Checker** Service to query by name, date-of-birth, or source list.
    ‚Ä¢ Applies Jaro-Winkler similarity scoring and returns a hit only if above our project threshold.
    ‚Ä¢ Must keep search latency < 500ms under peak (~200 reqs/sec) and stay within heap limits.
    ‚Ä¢ Csv input, parsing, and indexing are handled in `src/main/kotlin/watchlist/`.
    ‚Ä¢ Includes benchmark tests for both memory usage and search speed (in `src/jmh/kotlin/benchmark/`).

  **Focus Areas:**
  - **Performance & Scalability:** Ensure Lucene queries, lucene indexes, and similarity scoring meet <c500cms SLA.
  - **Memory Management:** Verify in-memory indexes stay within configured heap limits; check benchmark results.
  - **Similarity Logic:** Validate Jaro-Winkler threshold logic and avoid false positives/negatives.
  - **Index Integrity:** No data loss in CSV parsing; proper handling of nulls, encoding, and date formats.

  ## 1. PR Summary & Scope
  ‚Ä¢ Summarize the intent (new parser, index format change, search enhancements).
  ‚Ä¢ Highlight any CSV schema or Lucene index modifications.

  ## 2. Architecture & Indexing
  ‚Ä¢ Confirm separation of CSV parsing, indexing, and search modules (`.parser`, `.indexes`, `.api`).
  ‚Ä¢ Ensure Lucene `IndexWriter`/`Directory` usage is thread-safe and properly closed.

  ## 3. Search API & Contracts
  ‚Ä¢ Check request/response models in any controllers, ensure consistent API envelopes.
  ‚Ä¢ Filter parameters (DOB, source) must use correct types and validation annotations.

  ## 4. Performance & Benchmarks
  ‚Ä¢ Review benchmark configs in `benchmarks/`, ensure they reflect production heap and thread-pool settings.
  ‚Ä¢ Identify any potential hotspots or unbounded memory growth in index structures.

  ## 5. Similarity & Accuracy
  ‚Ä¢ Verify Jaro-Winkler or any other added Similiarty Scoring implementation and threshold constants.
  ‚Ä¢ Check unit tests around edge cases (name variations, diacritics, casing).

  ## 6. Testing & Reliability
  ‚Ä¢ Confirm CSV parsing tests cover large-file streaming without OOM.
  ‚Ä¢ Ensure integration tests include search under simulated load (e.g., using Gatling).
  ‚Ä¢ Validate JaCoCo / Kover coverage report: project-wide ‚â• 90 % and new/modified files ‚â• 85 %; flag any regressions.  
  ‚Ä¢ Require tests for all new public methods & critical paths; disallow unexplained @Disabled tests. 

  ## 7. Observability & Monitoring
  ‚Ä¢ Ensure metrics (search latency, index size) are emitted via Micrometer timers/gauges.
  ‚Ä¢ Logs for parse errors and search timeouts are present.

  ## 8. Security & Data Handling
  ‚Ä¢ Validate and sanitize all untrusted input to prevent Lucene query / path-traversal injection.  
  ‚Ä¢ Ensure new code respects existing auth / access-control boundaries; no privilege escalation.  
  ‚Ä¢ Keep secrets, keys, and connection strings in GitHub or vault-managed secrets ‚Äì never in code or logs.  
  ‚Ä¢ Verify third-party dependencies are up-to-date and free of known CVEs (automated scan acceptable).
  
    **Security Standards:**
    - OWASP Top 10 (2021): A01-A10 web application security
    - OWASP Top 10 LLM (2023/2025): AI-specific security risks
    - Company security guidelines: [CUSTOMIZE_WITH_YOUR_POLICIES]
    
    **Critical Analysis Areas:**
    1. Input Validation & Injection Prevention
    2. Authentication & Authorization
    3. Cryptographic Implementation
    4. Error Handling & Information Disclosure
    5. Security Configuration & Defaults
    6. Dependency & Component Security
    7. Sensitive Data Protection
    8. Design Security Flaws
    
    **Output Format:** Structured JSON with vulnerability details, OWASP categories, severity ratings, and remediation steps.

  ## 9. Documentation & ADRs
  ‚Ä¢ Update README with CSV source, index build steps, and benchmark instructions.
  ‚Ä¢ Add or update ADR if altering indexing strategy or similarity algorithm.

  ## 10. Actionable Feedback
  ‚Ä¢ List findings as üî¥/üü†/üü¢ with file paths or snippets, e.g.
    üî¥ `CSVParser.kt: potential OOM reading entire file into memory.`

  ## 11. Verdict
  ‚Ä¢ APPROVE / REQUEST_CHANGES / COMMENT_ONLY
  ‚Ä¢ Brief rationale (e.g., ‚ÄúREQUEST_CHANGES: similarity threshold edge-case missing test‚Äù).
  
  Please keep the tone **constructive**, **concise**, and output **Markdown** only.
